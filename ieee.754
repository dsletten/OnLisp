Double-precision float stores significand in 52 bits with implied H.O. bit of 1. (Sign/exponent take 12 bits)
The <SIGNIFICAND> (primary value) returned by INTEGER-DECODE-FLOAT contains this implicit H.O. bit.
For example:
pi => 3.141592653589793
(format t "~B~%" (integer-decode-float pi)) => 11001001000011111101101010100010001000010110100011000
(format t "~X~%" (integer-decode-float pi)) => 1921FB54442D18

Compare:
http://binaryconvert.com/result_double.html
3.141592653589793 => 0x400921FB54442D18

Sign Exponent      Significand
0    10000000000   1001001000011111101101010100010001000010110100011000
     400           921FB54442D18

Lisp:     11001001000011111101101010100010001000010110100011000
          1   9   2   1   F   B   5   4   4   4   2   D   1   8
IEEE-754:  1001001000011111101101010100010001000010110100011000
              9   2   1   F   B   5   4   4   4   2   D   1   8
-----------------------------------------------------------------
The exponent returned by INTEGER-DECODE-FLOAT differs from the IEEE-754 representation in 2 ways:
- The IEEE exponent is an "excess" value shifted by 1023, 127 for double-precision, single-precision respectively.
- The Lisp value is an integer, which scales the significand by 52, 23 places, respectively.

IEEE:
Float.MAX_VALUE
$25 ==> 3.4028235E38

Integer.toHexString(Float.floatToRawIntBits(Float.MAX_VALUE))
$30 ==> "7f7fffff"
Sign Exponent      Significand
0    11111110      11111111111111111111111
     FE            7FFFFF
     254           8388607

Exponent 254 -> (254 - 127) = 127

Thus, MOST-POSITIVE-SINGLE-FLOAT = ([1].11111111111111111111111)₂ X 2¹²⁷

Lisp:
most-positive-single-float => 3.4028235e38
(integer-decode-float most-positive-single-float)
16777215
104
1

7FFFFF -> FFFFFF (Implied bit) = 16777215
Significand: 1.11111111111111111111111 -> 111111111111111111111111 by shifting binary point by 23 places.

Thus, given IEEE exponent 254:
(254 - 127) - 23 = 104
-----------------------------------------------------------------
IEEE:
Double.MAX_VALUE
$32 ==> 1.7976931348623157E308

Long.toHexString(Double.doubleToRawLongBits(Double.MAX_VALUE))
$31 ==> "7fefffffffffffff"
Sign Exponent      Significand
0    11111111110   1111111111111111111111111111111111111111111111111111
     7FE           FFFFFFFFFFFFF
     2046          4503599627370495

Exponent 2046 -> (2046 - 1023) = 1023

Thus, MOST-POSITIVE-DOUBLE-FLOAT = ([1].1111111111111111111111111111111111111111111111111111)₂ X 2¹⁰²³

Lisp:
most-positive-double-float => 1.7976931348623157d308
(integer-decode-float most-positive-double-float)
9007199254740991
971
1

FFFFFFFFFFFFF -> 1FFFFFFFFFFFFF (Implied bit) = 9007199254740991
Significand: 1.1111111111111111111111111111111111111111111111111111 -> 11111111111111111111111111111111111111111111111111111 by shifting binary point by 52 places.

Thus, given IEEE exponent 2046:
(2046 - 1023) - 52 = 971
-----------------------------------------------------------------
* (= (+ 0.1d0) (* 1 0.1d0))
T
* (= (+ 0.1d0 0.1d0) (* 2 0.1d0))
T
* (= (+ 0.1d0 0.1d0 0.1d0) (* 3 0.1d0))
T
* (= (+ 0.1d0 0.1d0 0.1d0 0.1d0) (* 4 0.1d0))
T
* (= (+ 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0) (* 5 0.1d0))
T
* (= (+ 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0) (* 6 0.1d0))
NIL


* (rational (+ 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0))
5404319552844595/9007199254740992
* (rational  (* 6 0.1d0))
1351079888211149/2251799813685248
* (- * **)
1/9007199254740992

5404319552844595   1351079888211149   5404319552844596
---------------- < ---------------- = ----------------
9007199254740992   2251799813685248   9007199254740992



3 potentially separate values:
- 0.6d0
- (* 6 0.1d0)
- (+ 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0)


* (integer-decode-float (+ 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0))
5404319552844595
-53
1
* (integer-decode-float  (* 6 0.1d0))
5404319552844596
-53
1
* (integer-decode-float 0.6d0)
5404319552844595
-53
1


* (integer-decode-float (+ 0.1d0 0.1d0 0.1d0 0.1d0 0.1d0))
4503599627370496
-53
1
* (integer-decode-float (+ 0.1d0 0.1d0 0.1d0 0.1d0))
7205759403792794
-54
1
* (integer-decode-float (+ 0.1d0 0.1d0 0.1d0))
5404319552844596
-54
1
* (integer-decode-float (+ 0.1d0 0.1d0))
7205759403792794
-55
1
* (integer-decode-float 0.1d0)
7205759403792794
-56
1

jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.1))
$8 ==> "11111110111001100110011001100110011001100110011001100110011010"


jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.2))
$9 ==> "11111111001001100110011001100110011001100110011001100110011010"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.1 + 0.1))
$10 ==> "11111111001001100110011001100110011001100110011001100110011010"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(2 * 0.1))
$23 ==> "11111111001001100110011001100110011001100110011001100110011010"


jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.3))
$11 ==> "11111111010011001100110011001100110011001100110011001100110011"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.1 + 0.1 + 0.1))
$12 ==> "11111111010011001100110011001100110011001100110011001100110100"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(3 * 0.1))
$13 ==> "11111111010011001100110011001100110011001100110011001100110100"


jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.4))
$14 ==> "11111111011001100110011001100110011001100110011001100110011010"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.1 + 0.1 + 0.1 + 0.1))
$15 ==> "11111111011001100110011001100110011001100110011001100110011010"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(4 * 0.1))
$16 ==> "11111111011001100110011001100110011001100110011001100110011010"


jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.5))
$17 ==> "11111111100000000000000000000000000000000000000000000000000000"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.1 + 0.1 + 0.1 + 0.1 + 0.1))
$18 ==> "11111111100000000000000000000000000000000000000000000000000000"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(5 * 0.1))
$19 ==> "11111111100000000000000000000000000000000000000000000000000000"


jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.6))
$20 ==> "11111111100011001100110011001100110011001100110011001100110011"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1))
$21 ==> "11111111100011001100110011001100110011001100110011001100110011"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(6 * 0.1))
$22 ==> "11111111100011001100110011001100110011001100110011001100110100"



0.1
Long.toBinaryString(Double.doubleToRawLongBits(0.1))
==> "11111110111001100110011001100110011001100110011001100110011010"
Actually:
  11111110111001100110011001100110011001100110011001100110011010
0011111110111001100110011001100110011001100110011001100110011010
0 01111111011 1.1001100110011001100110011001100110011001100110011010


1.1001100110011001100110011001100110011001100110011010 X 2^-4
(/ 1 16d0) => 0.0625
11.001100110011001100110011001100110011001100110011010 X 2^-5
(/ 3 32d0) => 0.09375
110.01100110011001100110011001100110011001100110011010 X 2^-6
(/ 6 64d0) => 0.09375
1100.1100110011001100110011001100110011001100110011010 X 2^-7
(/ 12 128d0) => 0.09375
11001.100110011001100110011001100110011001100110011010 X 2^-8
(/ 25 256d0) => 0.09765625
110011.00110011001100110011001100110011001100110011010 X 2^-9
(/ 51 512d0) => 0.099609375


                                                            Expt
  1.1001100110011001100110011001100110011001100110011010    -4
+ 1.1001100110011001100110011001100110011001100110011010    -4
--------------------------------------------------------
 11.0011001100110011001100110011001100110011001100110100    -4
  1.1001100110011001100110011001100110011001100110011010(0) -3

0.2 (literal/sum/product)
0 01111111100 1.1001100110011001100110011001100110011001100110011010
0 01111111100 1.1001100110011001100110011001100110011001100110011010
0 01111111100 1.1001100110011001100110011001100110011001100110011010


  1.1001100110011001100110011001100110011001100110011010    -4
  1.1001100110011001100110011001100110011001100110011010    -4
+ 1.1001100110011001100110011001100110011001100110011010    -4
--------------------------------------------------------
 11.0011001100110011001100110011001100110011001100110100    -4
  1.1001100110011001100110011001100110011001100110011010    -4
--------------------------------------------------------
100.1100110011001100110011001100110011001100110011001110    -4
  1.0011001100110011001100110011001100110011001100110011(10)-2

0.3
0 01111111101 1.0011001100110011001100110011001100110011001100110011 <- Truncated
0 01111111101 1.0011001100110011001100110011001100110011001100110100 <- Rounded
0 01111111101 1.0011001100110011001100110011001100110011001100110100


0.4
0 01111111101 1.1001100110011001100110011001100110011001100110011010
0 01111111101 1.1001100110011001100110011001100110011001100110011010
0 01111111101 1.1001100110011001100110011001100110011001100110011010


0.5
0 01111111110 1.0000000000000000000000000000000000000000000000000000
0 01111111110 1.0000000000000000000000000000000000000000000000000000
0 01111111110 1.0000000000000000000000000000000000000000000000000000


   1.1001100110011001100110011001100110011001100110011010      -4

   1.1001100110011001100110011001100110011001100110011010      -4
   1.1001100110011001100110011001100110011001100110011010      -4
   1.1001100110011001100110011001100110011001100110011010      -4
   1.1001100110011001100110011001100110011001100110011010      -4
+  1.1001100110011001100110011001100110011001100110011010      -4
---------------------------------------------------------
1000.0000000000000000000000000000000000000000000000000         -4    (0.5)
+  1.1001100110011001100110011001100110011001100110011010      -4
---------------------------------------------------------
1001.1001100110011001100110011001100110011001100110011010      -4
   1.0011001100110011001100110011001100110011001100110011(010) -1

0.6
0 01111111110 1.0011001100110011001100110011001100110011001100110011 <- Truncated
0 01111111110 1.0011001100110011001100110011001100110011001100110011 <- Truncated
0 01111111110 1.0011001100110011001100110011001100110011001100110100 <- Rounded



0.6 = 2 * 0.3

* (= (* 2 0.3d0) (* 6 0.1d0))
NIL
* (integer-decode-float (* 2 0.3d0))
5404319552844595
-53
1
* (integer-decode-float (* 6 0.1d0))
5404319552844596
-53
1

0 01111111101 1.0011001100110011001100110011001100110011001100110011 0.3d0
0 01111111110 1.0011001100110011001100110011001100110011001100110011 (* 2 0.3d0)
0 01111111110 1.0011001100110011001100110011001100110011001100110100 (* 6 0.1d0)
-----------------------------------------------------------------
Infinity/NaN all exponent bits 1.
Infinity all significand bits 0:
Long.toHexString(Double.doubleToRawLongBits(Double.POSITIVE_INFINITY))
$33 ==> "7ff0000000000000"

Long.toHexString(Double.doubleToRawLongBits(Double.NEGATIVE_INFINITY))
$34 ==> "fff0000000000000"

NaN not all significand bits 0:
Long.toHexString(Double.doubleToRawLongBits(Double.NaN))
$35 ==> "7ff8000000000000"

long l = Double.doubleToRawLongBits(Double.NaN) 
l ==> 9221120237041090560

l++
l ==> 9221120237041090561

Double.longBitsToDouble(l)
$39 ==> NaN




Clojure notes:
Clojure has special numeric values for positive and negative infinity and not-a-number (NaN) to signal certain exceptional results:
(/ 1.0 0.0) => ##Inf
(/ -1.0 0.0) => ##-Inf
(/ 0.0 0.0) => ##NaN
But did you ever wonder what those values really were?
First, let's examine a typical double-precision float, e.g., pi:
Math/PI => 3.141592653589793
(Long/toHexString (Double/doubleToRawLongBits Math/PI)) => "400921fb54442d18"
This shows the machine representation of an IEEE-754 double.
Sign: 1 bit (0 = positive, 1 = negative)
Exponent: 11 bits ("1023 excess". Subtract 1023 to get true exponent)
Significand: 52 bits (Plus a 53rd "hidden" bit. The leading bit is always assumed to be 1)
Sign Exponent    Significand
0    10000000000 1001001000011111101101010100010001000010110100011000
Here, the exponent is 0b10000000000 = 1024.
Minus the "1023 excess" 1024 - 1023 = 1
Thus pi = 1.1001001000011111101101010100010001000010110100011000 X 2^1
Now the special values for infinity and NaN have a particular pattern. All of the exponent bits are 1.
For infinity, all of the significand bits are 0:
(Long/toHexString (Double/doubleToRawLongBits Double/POSITIVE_INFINITY))
"7ff0000000000000"
0 11111111111 0000000000000000000000000000000000000000000000000000

(Long/toHexString (Double/doubleToRawLongBits Double/NEGATIVE_INFINITY))
"fff0000000000000"
1 11111111111 0000000000000000000000000000000000000000000000000000
And for NaN, not all of the significand bits are 0:
(Long/toHexString (Double/doubleToRawLongBits Double/NaN))
"7ff8000000000000"
0 11111111111 1000000000000000000000000000000000000000000000000000
Any number that has this pattern is a NaN value:
(Double/doubleToRawLongBits Double/NaN) => 9221120237041090560
(inc (Double/doubleToRawLongBits Double/NaN)) => 9221120237041090561
(Double/longBitsToDouble (inc (Double/doubleToRawLongBits Double/NaN))) => ##NaN
(Long/toHexString (Double/doubleToRawLongBits *1)) => "7ff8000000000001"
So evidently there are 2^52 - 1 possible NaN values!
================================================================================
250310 月
Common Lisp inconsistent or Java wrong??

One of the places where Clojure's seams show the JVM underneath involves numerical operations.
Clojure supports arbitrary-precision integers, but only in some places. For example, bit shifting operations
only apply to Java Long instances:
(bit-shift-left 1 1) => 2
(bit-shift-left 1 2) => 4
(bit-shift-left 1 62) => 4611686018427387904
(bit-shift-left 1 63) => -9223372036854775808
(== Long/MIN_VALUE (bit-shift-left 1 63)) => true

Beyond that, the values wrap around:
(bit-shift-left 1 64) => 1
(bit-shift-left 1 65) => 2
vs.
(bit-shift-left (bit-shift-left 1 62) 1) => -9223372036854775808
(bit-shift-left (bit-shift-left (bit-shift-left 1 62) 1) 1) => 0

This odd wrapping behavior is consistent with Java:
jshell> 1L << 62
$58 ==> 4611686018427387904

jshell> 1L << 63
$59 ==> -9223372036854775808

jshell> 1L << 64
$60 ==> 1

Note the explicit designation `1L' above. Java and Clojure differ with
Java Integer values:
1 << 30 => 1073741824
1 << 31 => -2147483648
1 << 33 => 2
1 << 32 => 1
(bit-shift-left (int 1) 30) => 1073741824
(bit-shift-left (int 1) 31) => 2147483648
(bit-shift-left (int 1) 32) => 4294967296
(bit-shift-left (int 1) 33) => 8589934592

Apparently `bit-shift-left' automatically promotes its first argument:
(class (int 1)) => java.lang.Integer
(class (bit-shift-left (int 1) 30)) => java.lang.Long
(class (bit-shift-left (int 1) 31)) => java.lang.Long

This wrapping behavior is baked into how the compiler handles the << operator:
jshell> Long.MAX_VALUE << 62
$62 ==> -4611686018427387904

jshell> Long.MAX_VALUE << 63
$63 ==> -9223372036854775808

0111111111111111111111111111111111111111111111111111111111111111
|
|
v
1000000000000000000000000000000000000000000000000000000000000000 ; Almost gone...

jshell> Long.MAX_VALUE << 64
$64 ==> 9223372036854775807

jshell> Long.MAX_VALUE << 63 << 1
$65 ==> 0

But what about floating-point values? A left shift is equivalent to multiplying by a power of 2.
In other words, the significand of the floating-point number need not change:
jshell> Long.toHexString(Double.doubleToRawLongBits(123))
$80 ==> "405ec00000000000"

jshell> Long.toHexString(Double.doubleToRawLongBits(123 << 3))
$81 ==> "408ec00000000000"

jshell> Long.toHexString(Double.doubleToRawLongBits(123 * Math.pow(2, 3)))
$82 ==> "408ec00000000000"

jshell> Long.toBinaryString(Double.doubleToRawLongBits(123))
$75 ==> "100000001011110110000000000000000000000000000000000000000000000"
                    ^
jshell> Long.toBinaryString(Double.doubleToRawLongBits(123 << 3))
$76 ==> "100000010001110110000000000000000000000000000000000000000000000"
                    ^
jshell> Long.toBinaryString(Double.doubleToRawLongBits(123 * Math.pow(2, 3)))
$77 ==> "100000010001110110000000000000000000000000000000000000000000000"
                    ^
Now consider:
(bit-shift-left 1 62) => 4611686018427387904
(math/pow 2 62) => 4.611686018427388E18

The equality of these 2 values appears to be a special case for powers of 2. The integer value
merely shifts over the single 1 bit with many trailing 0s:
100000000000000000000000000000000000000000000000000000000000000

The double-precision float cannot represent all of those bits in its significand:
0 10000111101 0000000000000000000000000000000000000000000000000000

With the hidden bit, the significand is effectively
10000000000000000000000000000000000000000000000000000 (double)
vs.
100000000000000000000000000000000000000000000000000000000000000 (long)

So clearly these values are not equal in a strict sense.

But if the "missing" bits were assumed to be present as 0s then the equality of the two values is understood:
(== (bit-shift-left 1 62) (math/pow 2 62)) => true

Common Lisp agrees:
(= (ash 1 62) (expt 2d0 62)) => T

The following example seems to fit the same pattern:
(bit-shift-left 3 61)
(* 3 (math/pow 2 61)))

0 10000111101 1000000000000000000000000000000000000000000000000000

Once again we have a couple of meaningful bits followed by lots of 0s.
110000000000000000000000000000000000000000000000000000000000000 (long)
vs. 
11000000000000000000000000000000000000000000000000000 (double)

However, what about this?
(inc (bit-shift-left 3 61))
(inc (* 3 (math/pow 2 61))))

0 10000111101 1000000000000000000000000000000000000000000000000000

110000000000000000000000000000000000000000000000000000000000001 (long)
vs.
1000000000000000000000000000000000000000000000000000

The significand of the double value cannot represent the incremented bit.
Clearly these are 2 different numbers.

Clojure says that these 2 are equal (==):
(== (inc (bit-shift-left 3 61)) (inc (* 3 (math/pow 2 61))))

But Common Lisp disagrees:
(= (ash 3 61) (* 3 (expt 2d0 61))) => T
(= (1+ (ash 3 61)) (1+ (* 3 (expt 2d0 61)))) => NIL          甲

Both Common Lisp and Java demonstrate that the increment has no impact on the float value:
(integer-decode-float (* 3 (expt 2d0 61)))      => 6755399441055744; 10; 1
(integer-decode-float (1+ (* 3 (expt 2d0 61)))) => 6755399441055744; 10; 1

Double.doubleToRawLongBits(Math.pow(2, 61) * 3)     => 4888657395510673408
Double.doubleToRawLongBits(Math.pow(2, 61) * 3 + 1) => 4888657395510673408
(3l << 61) + 1 == Math.pow(2, 61) * 3 + 1 => true         !!!!

So the integer value is correctly incremented while the bit is lost in the float.
They clearly are not equal.

But is Common Lisp inconsistent?
(= (coerce (ash 3 61) 'double-float) (coerce (1+ (ash 3 61)) 'double-float)) => T

This coercion must be how Common Lisp tests the equality (甲)????
(= (coerce (1+ (ash 3 61)) 'double-float) (1+ (* 3 (expt 2d0 61)))) => T
(eql (1+ (* 3 (expt 2d0 61))) (coerce (1+ (ash 3 61)) 'double-float)) => T

(integer-decode-float (coerce (ash 3 61) 'double-float)) =>      6755399441055744; 10; 1
(integer-decode-float (coerce (1+ (ash 3 61)) 'double-float)) => 6755399441055744; 10; 1

(All implementations tested exhibit identical behavior: ABCL/CLISP/Clozure/CMUCL/SBCL)
